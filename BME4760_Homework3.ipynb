{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Part a\n",
    "a) Load the dataset in an iPython notebook \\[2 point\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: *Use the accompanied dataset for this homework. Read the dataset description below carefully and make sure you understand the dataset features and values. \n",
    "Dataset description: This dataset (Colon Cancer) contains expression levels of 2000 genes taken in 62 different samples. For each sample, it is indicated whether it came from a tumor biopsy or not (0/1). Note that the first column in the file corresponds to the label of the instance. See the Genes.txt file for description of genes and tissues.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "raw_data = pd.read_csv('HW3data.csv', header=None)\n",
    "raw_table = pd.read_table('Genes.txt', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = raw_table.loc[:, 0]\n",
    "temp = pd.DataFrame(['value'])\n",
    "header = pd.concat([temp, header])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    (value,)  (Hsa.3004,)  (Hsa.13491,)  (Hsa.13491,)  (Hsa.37254,)  \\\n",
      "0          0     2.080754      1.099069      0.927763      1.029081   \n",
      "1          1     1.109457      0.786453      0.445560     -0.146323   \n",
      "2          0    -0.676530      1.693100      1.559247      1.559983   \n",
      "3          1     0.534396      1.677537      1.489030      0.778605   \n",
      "4          0    -1.018903      0.511080      0.755641      1.013816   \n",
      "5          1    -1.185369     -0.514473     -0.566634      1.224720   \n",
      "6          0     1.779054      0.423947      0.820696      2.525687   \n",
      "7          1    -0.889638     -0.315453     -0.073131      1.157503   \n",
      "8          0    -0.659694     -0.184388     -0.540022      1.122418   \n",
      "9          1    -1.225801     -0.212615     -0.588923      1.335412   \n",
      "10         0    -0.377282     -2.620493     -2.763910      0.612038   \n",
      "11         1     1.639214      0.663551      0.324924      1.456457   \n",
      "12         0     0.125390      1.286657      1.505225      0.804243   \n",
      "13         1     0.055593      0.758397      0.857047      0.159382   \n",
      "14         0    -0.861022      0.432429      0.404421     -0.230533   \n",
      "15         1    -0.672304     -0.484720     -0.667529     -0.703183   \n",
      "16         0     1.605147     -0.771553     -0.819771      0.292317   \n",
      "17         1     0.867577      0.501470      0.242006      0.417230   \n",
      "18         0     0.590751      0.440645      0.585110      1.470657   \n",
      "19         1    -0.311476     -1.482608     -1.697240      1.728988   \n",
      "20         0     0.369171      0.742324      0.584255      0.617444   \n",
      "21         1     0.137575      0.801932      0.673334      0.089943   \n",
      "22         0    -1.163365     -1.214555     -1.359716     -1.237726   \n",
      "23         1     0.907830      1.640852      1.416463      0.953309   \n",
      "24         0     0.261661     -0.126443      0.084908     -0.461416   \n",
      "25         0     1.070146      0.393399      0.695527      0.180488   \n",
      "26         0     1.440709      0.649996      0.873454      0.190425   \n",
      "27         0     0.603603      1.226511      1.338774      0.105289   \n",
      "28         0     0.668765      0.444708      0.397101      0.643610   \n",
      "29         0     1.196460      0.259518      0.023820      0.661112   \n",
      "..       ...          ...           ...           ...           ...   \n",
      "32         0     0.704230     -1.266950     -1.082385      0.282644   \n",
      "33         0     0.451197      1.533083      1.493214      0.108655   \n",
      "34         0     0.173573     -0.086988      0.049312      0.154649   \n",
      "35         0    -0.552768     -0.394988     -0.067447     -1.468454   \n",
      "36         0    -1.633654      1.224956      1.036479      1.108610   \n",
      "37         0    -1.154575     -1.139515     -0.852773     -1.805587   \n",
      "38         1    -1.765218      0.434114      0.397484     -2.180245   \n",
      "39         0     1.483438      0.567198      0.573307     -0.202935   \n",
      "40         0    -0.953049      1.107939      1.328478     -1.644577   \n",
      "41         1     0.214162      0.512283      0.318795      0.375563   \n",
      "42         1     0.426636     -1.864192     -1.847756     -0.258318   \n",
      "43         0     0.781703     -2.166325     -1.935412     -0.591776   \n",
      "44         0    -0.333621     -0.331659     -0.039726     -0.684055   \n",
      "45         0     0.798422     -2.030147     -1.800778     -0.435794   \n",
      "46         0    -2.345574     -1.266346     -1.295295     -1.866567   \n",
      "47         1    -0.793556     -0.764538     -0.615078     -1.524149   \n",
      "48         0     1.016578      0.165638     -0.216089     -0.323165   \n",
      "49         1    -0.810990     -0.344152     -0.396643     -0.912228   \n",
      "50         1    -0.082485     -0.068667      0.054522     -0.487776   \n",
      "51         0    -0.215307     -0.078671      0.099442     -1.576599   \n",
      "52         0     0.177628      0.549644      1.016878     -1.188864   \n",
      "53         1    -1.819608      0.087764      0.334434     -0.566902   \n",
      "54         1    -0.149788     -0.537464     -0.865811     -0.439747   \n",
      "55         0    -0.642749     -0.646193     -0.924490     -0.390230   \n",
      "56         0    -2.129373      0.025128      0.007812     -0.510764   \n",
      "57         0     0.488711      0.764970      0.920233     -0.958693   \n",
      "58         0    -0.146703      0.112155      0.282926     -1.135509   \n",
      "59         1     0.200706     -0.668545     -0.935933     -0.886087   \n",
      "60         0     0.829749      0.299447      0.130738      0.335530   \n",
      "61         1     0.566684     -0.533660     -0.761951     -0.087032   \n",
      "\n",
      "    (Hsa.541,)  (Hsa.20836,)  (Hsa.1977,)  (Hsa.44472,)  (Hsa.3087,)  \\\n",
      "0    -0.130763      1.265460    -0.436286      0.728881     2.107979   \n",
      "1    -0.996316      0.555759     0.290734     -0.145259     1.132660   \n",
      "2    -0.982179     -1.358507    -1.313994     -0.455067     0.295214   \n",
      "3    -0.183776     -1.116850    -1.487557     -0.579511     0.292683   \n",
      "4     0.529899      0.160440    -0.087055      1.295290     0.458736   \n",
      "5     0.619244     -0.684713    -0.798129      1.368770    -0.697007   \n",
      "6     0.666921      0.661346     0.425365      0.165247     1.967905   \n",
      "7    -0.311039     -0.364472    -1.621636      1.192999     0.689805   \n",
      "8     0.562609     -2.988315    -2.349808     -1.325007    -0.017002   \n",
      "9    -0.356505      0.354394     0.699607      0.190782    -0.139117   \n",
      "10   -0.155718     -1.456066     0.683292     -1.031567     0.718861   \n",
      "11   -0.170587     -0.443486    -0.832590     -0.928334     1.320070   \n",
      "12    1.384430      0.633277     1.682128      1.924996     0.072503   \n",
      "13    1.020294      0.733855     1.141088      1.213815    -0.236974   \n",
      "14    0.492867      0.274014    -0.873134      0.609713    -0.112962   \n",
      "15    1.872858      1.295695     1.044664     -1.255296    -0.686953   \n",
      "16    1.648191      0.192798     1.348876      1.284650     1.989654   \n",
      "17    1.723881     -1.249272    -0.815193      2.015221     1.160137   \n",
      "18    0.724532     -1.874136    -2.038916      1.115607     1.225129   \n",
      "19    0.511898     -0.943735    -0.285187      1.028791    -0.365555   \n",
      "20   -1.513952      1.180225    -0.268009      0.513435     0.428941   \n",
      "21   -1.572563      0.878675     0.506696      0.222351     1.146443   \n",
      "22    1.509944      0.462035     0.989522      1.017890    -1.057706   \n",
      "23    2.895000      0.322749     1.517181      2.689922     0.509099   \n",
      "24    0.420518      0.386768     0.257387      1.184280    -0.059124   \n",
      "25   -0.539207      0.520514     1.279115     -0.856697     0.544119   \n",
      "26   -0.368266      1.162666     1.748127     -0.859296     1.142555   \n",
      "27    0.012994      0.735462     1.281292      0.208414     0.605721   \n",
      "28   -1.578104      0.852593     1.579514     -1.791649     0.432917   \n",
      "29   -0.757198      0.931691     0.942438     -1.034322     1.033307   \n",
      "..         ...           ...          ...           ...          ...   \n",
      "32    0.599121      1.176305     0.821073      0.267318     0.377003   \n",
      "33    0.118696      0.359560    -0.080691      1.088030     0.041763   \n",
      "34   -0.854644      0.395668    -0.501723     -0.606768     0.427740   \n",
      "35    0.986678     -0.950956    -0.564847      0.906280    -0.336318   \n",
      "36   -2.121205      0.087452     0.285415     -0.453603     0.292197   \n",
      "37    1.211636     -0.043248     1.020932      0.475199    -0.733753   \n",
      "38   -0.482221      0.579913     0.007180     -0.396361    -2.186182   \n",
      "39    1.108224      1.627166     1.202437     -0.093951     0.779752   \n",
      "40   -0.074236     -0.776688    -1.405831     -0.338663    -0.583577   \n",
      "41   -0.319845     -1.907147    -1.601023      0.697753    -0.008599   \n",
      "42   -1.380025     -0.065725    -0.268582     -0.769468    -2.508847   \n",
      "43   -0.140932     -1.005584    -0.891690     -0.254486     0.518198   \n",
      "44   -1.182427      1.274619     0.962359     -1.101843    -0.192507   \n",
      "45    0.044236     -1.160167    -0.552252      0.297195    -0.647570   \n",
      "46   -0.665226     -0.325585    -0.164589     -0.191533    -1.855450   \n",
      "47   -1.693305     -1.622770    -0.292296     -0.927984    -1.256091   \n",
      "48   -1.256994      0.634088     0.721427     -1.759378     0.474816   \n",
      "49    0.301997      0.567269     0.458596     -0.449325    -0.141738   \n",
      "50    0.311532      1.165237     0.297936     -0.143873    -0.156585   \n",
      "51    1.214732     -0.403080    -0.901705      0.446720    -0.046425   \n",
      "52    0.908239      0.043510    -1.113660      0.489169    -0.061382   \n",
      "53    0.127363     -0.683805    -1.144701      0.270141    -1.706886   \n",
      "54   -1.324960     -1.683660    -0.189472     -0.767448    -0.406531   \n",
      "55   -0.193051     -0.457995    -0.024138     -0.516478    -2.929511   \n",
      "56   -0.418168     -1.437657    -1.760120     -0.905048    -1.363562   \n",
      "57   -0.147683      0.588944     0.097142     -0.212724    -0.827331   \n",
      "58    0.683854      1.449191     0.963985      0.394901    -0.543475   \n",
      "59   -1.372156      0.128890     0.488561     -2.447959    -0.445897   \n",
      "60   -0.401385      0.495544    -0.646200     -1.041644    -0.267708   \n",
      "61   -0.283129      0.832089    -0.330326     -0.583525    -0.258754   \n",
      "\n",
      "       ...       (Hsa.2618,)  (Hsa.27285,)  (Hsa.41260,)  (Hsa.14822,)  \\\n",
      "0      ...         -0.825403     -0.138451      0.382957      0.876697   \n",
      "1      ...         -1.056288     -0.205499     -1.815374      0.324373   \n",
      "2      ...          1.242968      1.230157     -2.038999      2.366093   \n",
      "3      ...          0.559852     -0.593149     -4.440577      1.720705   \n",
      "4      ...          0.227110      0.497628     -0.083921     -0.382733   \n",
      "5      ...          0.926855      0.302304      0.302785     -1.170087   \n",
      "6      ...          0.284194     -0.555516     -1.455897     -0.240853   \n",
      "7      ...         -0.113531     -0.485290     -0.094577     -0.809093   \n",
      "8      ...         -0.819396     -0.952811      0.065197      0.985648   \n",
      "9      ...          1.772469      0.804056     -0.361595     -0.975587   \n",
      "10     ...          0.115201      1.137555      0.306306     -1.807855   \n",
      "11     ...          0.881965     -0.772486      0.048502     -0.290692   \n",
      "12     ...          0.668764      0.795207     -0.141979     -0.328349   \n",
      "13     ...          0.425991      0.226252     -1.182270      0.945301   \n",
      "14     ...          0.922434     -0.586524      0.897438      0.538331   \n",
      "15     ...          1.053262     -0.570598      0.428686      0.517352   \n",
      "16     ...         -4.302872     -0.364189      1.121105     -0.628776   \n",
      "17     ...         -1.126207     -0.030568     -0.288405     -1.828125   \n",
      "18     ...         -0.525329     -0.934040      0.008399     -0.476887   \n",
      "19     ...          1.184450      0.515086     -0.388818     -2.137428   \n",
      "20     ...         -0.517142     -0.494955     -0.748101     -0.151951   \n",
      "21     ...         -0.306509      0.009629     -1.194875      0.492434   \n",
      "22     ...          0.365183      0.282023      1.659938     -1.356941   \n",
      "23     ...          0.951224      1.332461      0.269523     -2.526484   \n",
      "24     ...         -0.320691     -0.090327     -0.144941      0.917479   \n",
      "25     ...         -0.561481      1.508077      0.556502     -0.053134   \n",
      "26     ...          0.246977      1.430460      0.490623     -0.069066   \n",
      "27     ...         -1.044283      0.531571      0.045947      0.543013   \n",
      "28     ...         -0.375764      1.895205     -0.087835      0.779827   \n",
      "29     ...         -0.051668      0.675375      0.032776      0.177638   \n",
      "..     ...               ...           ...           ...           ...   \n",
      "32     ...          0.292064      0.478941      1.103350     -1.254981   \n",
      "33     ...         -0.909571      0.401607      0.124062      0.408449   \n",
      "34     ...         -1.181713     -0.374841     -0.158974     -0.380094   \n",
      "35     ...         -1.074213      0.442287      0.900869     -1.051292   \n",
      "36     ...          0.428210      1.913617     -1.500744      1.833650   \n",
      "37     ...         -1.735276      1.296925      1.524022     -0.917120   \n",
      "38     ...          0.686778      0.644273      0.479698      0.975548   \n",
      "39     ...         -0.525945      0.750996      0.935524     -0.132938   \n",
      "40     ...         -0.736461     -3.156028      0.569853      0.636454   \n",
      "41     ...         -0.016903     -0.256217      0.069957     -0.059088   \n",
      "42     ...          0.315847     -1.995580     -1.221399     -0.010682   \n",
      "43     ...         -0.941889      0.144534     -0.096478     -0.452878   \n",
      "44     ...          1.483875      0.481589     -0.187037      0.417466   \n",
      "45     ...         -0.809573      0.062506      0.593717     -0.809891   \n",
      "46     ...         -1.267359     -0.837099      0.349589     -0.664065   \n",
      "47     ...          1.713019      1.207519     -0.935843     -0.063432   \n",
      "48     ...          0.049964      0.071878     -0.445551      0.427269   \n",
      "49     ...          0.617989      0.098371      0.649645      0.846294   \n",
      "50     ...          1.510051     -1.349517     -0.659209     -0.745793   \n",
      "51     ...         -0.525371     -0.379801      1.236858     -0.521071   \n",
      "52     ...         -0.653858     -1.370089      1.474176      0.456790   \n",
      "53     ...          0.509312     -0.430256      0.072601      0.638787   \n",
      "54     ...         -0.064966      0.413162     -0.195991      0.312849   \n",
      "55     ...          0.212534      0.424437     -0.027541      0.501833   \n",
      "56     ...          1.481529     -1.041608      1.329088      2.126108   \n",
      "57     ...         -0.319253     -2.880039      0.322515      1.150149   \n",
      "58     ...         -0.544976     -1.315633      0.404797     -0.364126   \n",
      "59     ...          0.846102      0.514963     -1.398877      0.771972   \n",
      "60     ...          0.843415      0.436470      0.082887     -0.121180   \n",
      "61     ...          0.151594     -0.540061      0.915846      1.025732   \n",
      "\n",
      "    (Hsa.336,)  (Hsa.984,)  (Hsa.35124,)  (Hsa.3952,)  (Hsa.32734,)  \\\n",
      "0    -0.216234   -1.408300      0.393327    -0.148522      1.591533   \n",
      "1    -1.296909   -0.870757      1.108739     1.094010     -0.492141   \n",
      "2     0.820656    1.404501      0.176860    -0.086285     -0.390878   \n",
      "3    -0.124617   -0.435880      0.228440    -0.893938      1.005879   \n",
      "4    -0.913389    1.122928      0.834571    -0.283786     -2.860340   \n",
      "5    -1.189162    0.418884     -2.247347    -0.455263     -1.528083   \n",
      "6     0.249138   -0.915572     -0.764530    -0.808293      0.003886   \n",
      "7     0.453226   -0.009268     -0.872640    -1.730826      0.665384   \n",
      "8     0.745522    0.508879      0.846929     0.644346      0.156497   \n",
      "9    -0.652528   -0.106173     -0.613650    -0.886065      0.645764   \n",
      "10    0.899761    0.750847      1.847261     1.500090      1.013120   \n",
      "11   -0.970130   -0.062976      1.019466     0.998675     -0.112029   \n",
      "12   -0.111059    0.443720     -1.034155    -1.383400     -0.958857   \n",
      "13   -0.297952   -1.460249     -0.177619    -1.151216      0.722640   \n",
      "14   -1.281134    0.539094     -0.343020    -0.269283      0.003406   \n",
      "15   -0.052869   -1.813498     -1.282465    -1.565887     -1.291665   \n",
      "16   -1.452871    1.065736      0.929366     0.850532      0.092422   \n",
      "17    0.378633   -0.861010      0.567541    -0.237284      1.509842   \n",
      "18   -1.086239    0.437081     -0.718286     0.206024     -2.804114   \n",
      "19   -0.067351    0.266181     -0.481120    -1.212296      0.778511   \n",
      "20   -0.330637   -1.791074      1.330228     0.145018      0.413500   \n",
      "21   -0.095780   -1.453297      0.351192     0.005632      1.104979   \n",
      "22   -0.905060    0.402223     -0.248060    -0.563791      0.396050   \n",
      "23   -3.164198   -0.381382     -0.340095    -2.461643     -0.937018   \n",
      "24   -1.117696   -3.595862     -0.160189     0.325858     -0.101886   \n",
      "25    0.866155   -0.904096     -0.246096     0.980219     -1.537980   \n",
      "26    0.458956   -0.929896      0.631799     0.350061     -1.761495   \n",
      "27    0.840315   -0.355442      0.196545     0.230857      0.709464   \n",
      "28    2.017198   -0.408198      0.239587     0.898247     -0.269794   \n",
      "29    1.646497   -0.684288      0.154495     1.474240      0.517323   \n",
      "..         ...         ...           ...          ...           ...   \n",
      "32    1.061866   -1.294651      1.048844    -0.254242      1.365350   \n",
      "33   -0.493232    0.925940     -0.922493    -0.144082     -0.839826   \n",
      "34   -0.336835   -0.181890      1.158283     0.455972      0.647447   \n",
      "35   -0.816867    0.148245      0.218910    -0.104252     -1.009324   \n",
      "36    2.854865   -0.437884      0.005739     0.008833     -0.252912   \n",
      "37   -0.044104   -0.425193      0.301845    -0.566719     -1.419485   \n",
      "38   -1.429519    0.173506     -1.959468    -2.870571     -0.833660   \n",
      "39    0.527780    0.292531      0.870360     1.056712     -0.318613   \n",
      "40   -1.537726    0.639513     -0.232347    -0.737874      0.808167   \n",
      "41    0.796956   -0.131166     -0.766188    -0.311352      0.335782   \n",
      "42    1.343143    0.735802      1.460692     1.276620      1.355918   \n",
      "43    1.222409    1.115541      1.856981     1.660051      0.620906   \n",
      "44    0.055298   -1.201560      0.622394     0.003722      1.009665   \n",
      "45    0.433926    0.079638      1.239839     1.148068     -0.024702   \n",
      "46   -0.684032    0.095660      0.941494    -0.649761     -0.640791   \n",
      "47   -0.138106    0.889653     -0.644733    -0.986025     -0.728026   \n",
      "48    0.003540   -0.084864      0.646203     1.266569      1.241043   \n",
      "49   -0.185618    1.041692     -0.641211    -0.301740     -0.311732   \n",
      "50    0.929339   -0.796802     -0.180983     0.370376      0.989733   \n",
      "51    0.354033   -0.250985     -0.470433     0.072394      1.035807   \n",
      "52   -1.054447    0.408274     -1.856790    -0.888702      0.519268   \n",
      "53   -0.419346   -0.109626     -1.700997    -1.697602     -0.191326   \n",
      "54    0.637454    1.503764      0.579904     1.632508     -0.723773   \n",
      "55    0.482323    0.604469     -0.861672     0.024896     -0.531409   \n",
      "56    0.011270    1.987943     -2.116854     0.712721      1.315854   \n",
      "57    1.132339    0.437734      0.174756     0.511431     -0.685264   \n",
      "58   -1.347496    0.075607     -2.387867     0.347011     -1.013400   \n",
      "59    0.203989   -0.214999      0.585247     0.605018      1.344914   \n",
      "60    0.407349    0.664057      0.533698     0.985630     -0.210661   \n",
      "61    0.786296    0.870661      0.154527    -0.453840      0.194535   \n",
      "\n",
      "    (Hsa.9683,)  \n",
      "0     -0.217481  \n",
      "1     -1.554079  \n",
      "2     -0.089465  \n",
      "3     -0.631247  \n",
      "4      0.280871  \n",
      "5      0.327891  \n",
      "6     -0.379341  \n",
      "7      0.977573  \n",
      "8     -0.147716  \n",
      "9      0.142760  \n",
      "10     0.067917  \n",
      "11    -0.224523  \n",
      "12    -0.020107  \n",
      "13    -0.044954  \n",
      "14     1.213104  \n",
      "15     1.190718  \n",
      "16     1.670912  \n",
      "17     2.055111  \n",
      "18     0.198566  \n",
      "19     0.688857  \n",
      "20    -0.629505  \n",
      "21     0.153397  \n",
      "22     2.362042  \n",
      "23     1.307743  \n",
      "24     0.515232  \n",
      "25    -0.623124  \n",
      "26    -0.670612  \n",
      "27    -0.798043  \n",
      "28    -0.593288  \n",
      "29    -1.032721  \n",
      "..          ...  \n",
      "32     0.153579  \n",
      "33    -1.210314  \n",
      "34     0.763341  \n",
      "35     1.331196  \n",
      "36    -2.614152  \n",
      "37     1.953458  \n",
      "38     0.192293  \n",
      "39    -0.063503  \n",
      "40    -1.221314  \n",
      "41    -0.412429  \n",
      "42     0.721191  \n",
      "43     0.302660  \n",
      "44    -0.235593  \n",
      "45     0.480412  \n",
      "46    -0.034518  \n",
      "47    -1.385885  \n",
      "48    -1.336079  \n",
      "49    -0.051559  \n",
      "50     0.067339  \n",
      "51     0.229729  \n",
      "52     0.008556  \n",
      "53     0.153198  \n",
      "54    -2.451679  \n",
      "55     0.408755  \n",
      "56     0.462371  \n",
      "57    -0.604012  \n",
      "58    -0.013589  \n",
      "59    -2.485066  \n",
      "60    -0.391249  \n",
      "61     0.064150  \n",
      "\n",
      "[62 rows x 2001 columns]\n"
     ]
    }
   ],
   "source": [
    "raw_data.columns = header\n",
    "print(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Part b\n",
    "b) Feature selection is an important machine-learning task that allows us to select the most important features in a given dataset. Scikit-learn provides multiple methods for choosing the best features. Use the Recursive Feature Elimination method (REF) with crossvalidation [here](http://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py), and show a plot to demonstrate the performance versus number of selected features \\[11 points\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buidling a classification task\n",
    "X, Y = make_classification(n_samples=1000,\n",
    "                           n_features=62,\n",
    "                           n_informative=3,\n",
    "                           n_redundant=2,\n",
    "                           n_repeated=0,\n",
    "                           n_classes=8,\n",
    "                           n_clusters_per_class=1,\n",
    "                           random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an RFE object\n",
    "svc = SVC(kernel=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RFECV(cv=StratifiedKFold(n_splits=2, random_state=None, shuffle=False),\n",
       "   estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "   n_jobs=1, scoring='accuracy', step=1, verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running accuracy scoring and cross validation\n",
    "rfecv = RFECV(estimator=svc,\n",
    "              step=1,\n",
    "              cv=StratifiedKFold(2),\n",
    "              scoring='accuracy')\n",
    "\n",
    "# Fitting model\n",
    "rfecv.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting performance vs number of selected features\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework part c\n",
    "c) Use the holdout method for testing using only the selected features. Report  the performance. \\[5 points\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\skyfr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup values and label\n",
    "X_value = raw_data.iloc[:, 1:2001].values # Values for gene expression\n",
    "y_value = raw_data.iloc[:, 0].values # Labels for gene markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make training and test data\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X_value,\n",
    "                     y_value,\n",
    "                     test_size=0.20,\n",
    "                     stratify=y_value,\n",
    "                     random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (49, 2000)\n",
      "Y_train shape:  (49,)\n",
      "X_test shape:  (13, 2000)\n",
      "Y_test shape:  (13,)\n"
     ]
    }
   ],
   "source": [
    "# Explore the dimensions of training and testing data.\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"Y_train shape: \", y_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"Y_test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RFECV(cv=StratifiedKFold(n_splits=2, random_state=None, shuffle=False),\n",
       "   estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "   n_jobs=1, scoring='accuracy', step=1, verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting model\n",
    "rfecv.fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting performance vs number of selected features\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy scores: [0.66666667 0.66666667 0.8        0.8        0.8        0.8\n",
      " 1.         1.         1.         1.        ]\n",
      "CV accuracy: 0.853 +/- 0.129\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# effective way to write cross-validation, jobs = number of CPU's you will run if available\n",
    "scores = cross_val_score(estimator=svc,\n",
    "                         X=X_train,\n",
    "                         y=y_train,\n",
    "                         cv=10,\n",
    "                         n_jobs=1)\n",
    "print('CV accuracy scores: %s' % scores)\n",
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework part d\n",
    "d) Create a GitHub repository and share your code via GitHub with the instructor by submitting the link on Canvas \\[2 points\\]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Github code here: https://github.com/mdelbasha/BME4760_HW3.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
